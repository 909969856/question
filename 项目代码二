#=======================================================================================#
#                                   库函数导入                                          #
#=======================================================================================#
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectPercentile, f_classif
import numpy as np
import sys
sys.path.append("final_project/")
from feature_format import featureFormat, targetFeatureSplit
from tester import test_classifier,dump_classifier_and_data
import pickle
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
#=======================================================================================#
#                                 把字典转化成数组                                      #
#=======================================================================================#
features_list = ['poi','salary',\
                'total_stock_value','total_payments','to_messages','long_term_incentive',\
                'exercised_stock_options','deferred_income','deferral_payments','bonus'] 
#features_list = ['poi','salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options',\
                 #'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred',\
                 #'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', \
                 #'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person']


with open("final_project/final_project_dataset.pkl","r") as data_file:
    data_dict = pickle.load(data_file)


my_dataset = data_dict
'''
data_temp =[]
for name in my_dataset:
    temp_list = []
    for feature in features_list:
        if my_dataset[name][feature]=='NaN':
            my_dataset[name][feature]=0
        temp_list.append(float(my_dataset[name][feature]))
    data_temp.append(temp_list)
data = np.array(data_temp)'''
data = featureFormat(my_dataset, features_list, sort_keys = True)
#=========================================================================================#
#                               把特征和标签分开                                          #
#=========================================================================================#
features = []
labels = []

for i in data:
    labels.append(i[0])
    features.append(i[1:])

#=========================================================================================#
#                    用Pipeline把PCA主成分提取和分类器装到一起                            #
#=========================================================================================#
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.naive_bayes import GaussianNB
from sklearn import svm
from sklearn import tree



pca = PCA(n_components=3)
clf = GaussianNB()
pca_GaussianNB = Pipeline([('pca', pca), ('GaussianNB', clf)])

#clf = svm.SVC(kernel = 'rbf',C = 10)
#pca_svm = Pipeline([('pca', pca), ('svc', clf)])

#clf = tree.DecisionTreeClassifier(min_samples_split=10)
#pca_tree = Pipeline([('pca', pca), ('tree', clf)])
    
#=========================================================================================#
#                            分层随机抽样交叉验证                                         #
#=========================================================================================#
#from sklearn.cross_validation import StratifiedShuffleSplit#旧版本
#sss = StratifiedShuffleSplit(labels, 1000, test_size = 0.1,random_state=0)
#for train,test in sss:
from sklearn.model_selection import StratifiedShuffleSplit#新版本
sss = StratifiedShuffleSplit(n_splits=1000, test_size=0.1, random_state=42)
accuracy_score_all = []
precision_score_all = []
recall_score_all = []
for train,test in sss.split(features,labels):
    features_train = [features[i] for i in train]
    labels_train = [labels[i]for i in train]
    features_test = [features[i]for i in test]
    labels_test = [labels[i]for i in test]
    
    pca_GaussianNB.fit(features_train,labels_train)
    pred = pca_GaussianNB.predict(features_test)
    accuracy_score_all.append(accuracy_score(pred,labels_test))
    precision_score_all.append(precision_score(labels_test, pred, average='macro'))
    recall_score_all.append(recall_score(labels_test, pred, average='macro'))
    
    #pca_svm.fit(features_train,labels_train)
    #print pca_svm.score(features_test,labels_test)
    
    #pca_tree.fit(features_train,labels_train)
    #print pca_tree.score(features_test,labels_test)
    
accuracy = np.array(accuracy_score_all).mean()
precision = np.array(precision_score_all).mean()
recall = np.array(recall_score_all).mean()
print 'accuracy:',accuracy
print 'precision:',precision
print 'recall:',recall


#=========================================================================================#
#                        存储分类器，数据集，和特征列表                                   #
#=========================================================================================#
dump_classifier_and_data(pca_GaussianNB, my_dataset, features_list)
#dump_classifier_and_data(pca_svm, my_dataset, features_list)
#dump_classifier_and_data(pca_tree, my_dataset, features_list)
#=========================================================================================#
#            算法性能：当使用 tester.py 评估性能时，精确度、召回率均至少为 0.3            #
#=========================================================================================#
test_classifier(pca_GaussianNB, my_dataset, features_list, folds = 1000)
#test_classifier(pca_svm, my_dataset, features_list, folds = 1000)
#test_classifier(pca_tree, my_dataset, features_list, folds = 1000)
